getwd()
getwd()
setwd("C:/Users/138070/OneDrive - Arrow Electronics, Inc/Desktop")
setwd("C:/Users/138070/Dropbox/MBA")
setwd("C:/Users/138070/Dropbox/MBA/Mod7")
# we are going to be importing and exporting data, so we need to know where to find that data;
# start by getting your working directory (the place where our data will be imported from or exported to):
getwd()
setwd("C:/Users/138070/Dropbox/MBA/Mod7")
install.packages("twitteR")
install.packages("plyr")
install.packages("stringr")
install.packages("tm")
install.packages("wordcloud")
install.packages("SnowballC")
library(twitteR)
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
library(SnowballC)
positiveWords = c("good", "best", "great")
negativeWords = c("bad", "worse", "worst")
class(postiveWords)
class(positiveWords)
negativeWords
positiveWords
positiveWords = readLines("positiveWords.txt")
negativeWords = readLines("negativeWords.txt")
negativeWords
positiveWords
generalCleaning = function(dataToClean)
{
dataToReturn = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", dataToClean) # remove hyperlinks
dataToReturn = gsub("<.*?>", "", dataToReturn) # remove HTML
dataToReturn = gsub("[^[:graph:]]", " ", dataToReturn) # anything not alphabetic and punctuation (e.g. emojis, pictures, videos)
dataToReturn = gsub("[[:punct:]]", "", dataToReturn) # remove punctuation
dataToReturn = gsub("[[:cntrl:]]", " ", dataToReturn) # remove control characters
dataToReturn = gsub("\\d+", "", dataToReturn) # remove numbers
dataToReturn = gsub("[[:space:]]", " ", dataToReturn) #remove "white space" characters, noting " " vs ""
dataToReturn = gsub("“", "", dataToReturn) #remove specific characters
dataToReturn = gsub("‘", "", dataToReturn) #remove specific characters
dataToReturn = gsub("”", "", dataToReturn) #remove specific characters
dataToReturn = gsub("  ", " ", dataToReturn) #remove specific characters
#example for later in the activity
#dataToReturn = gsub("äóî", "", dataToReturn) #remove specific characters
dataToReturn = tolower(dataToReturn) # notice how we make it lowercase before returning the data
return(dataToReturn)
}
calculateSentiment = function(dataToScore, dataToDisplay, positiveListOfWords, negativeListOfWords, nameOfDataset)
{
# for every row of data in dataToScore we will calculate the sentiment score, then build a list of scores
#   to list next to dataToDisplay:
listOfScores = laply(dataToScore, function(singleRowOfData, positiveListOfWords, negativeListOfWords) {
words = unlist(str_split(singleRowOfData, '\\s+')) #generates a list of all words in the row of data
# next, generate a list indicating "true" for every word in the list of "words" that is also in
#   positiveListOfWords, otherwise indicates "false":
positiveMatches = !is.na(match(words, positiveListOfWords))
negativeMatches = !is.na(match(words, negativeListOfWords)) #same idea, but for negativeListOfWords
# sum will count up all instances of "true" as 1, so in this case, we are counting the positiveMatches
#   then subtracting from negativeMatches:
scoreForSingleRow = sum(positiveMatches) - sum(negativeMatches)
return(scoreForSingleRow)
}, positiveListOfWords, negativeListOfWords, .progress="text" )
# remove emojis, pictures, videos, etc from our output (notice, dataToDisplay did not run through our
#   general cleaning)
dataToDisplay = gsub("[^[:graph:]]", " ", dataToDisplay)
# create a dataframe which will be the required data structure used to create a CSV (comma separated value) file;
#   basically, return three columns, "which dataset, sentiment and text" with data listed under each:
dataToReturn = data.frame("whichDataset"=nameOfDataset, sentiment=listOfScores, text=dataToDisplay)
return(dataToReturn)
}
####### Here is where you continue - let's read the "jeepTweetsExample.csv" into "myTweets"
myTweets = read.csv("jeepTweetsExample.csv")
# different ways to look at some of the data within the myTweets data
myTweets
# what is the data structure of tweets?
class(myTweets)
# display the internal data structure of tweets
str(myTweets)
# look at an individual column of data in tweets
myTweets$text
# you could load them directly from those locations (assuming your computer can access the URLs), as a pre-caution,
#   the 2 datasets have been provided so you can load them from your working directory:
myTweets1 = read.csv(BarackObama.csv)
# you could load them directly from those locations (assuming your computer can access the URLs), as a pre-caution,
#   the 2 datasets have been provided so you can load them from your working directory:
myTweets1 = read.csv("BarackObama.csv")
myTweets2 = read.csv("realDonaldTrump.csv")
str(myTweets)
summary(myTweets)
myTweets2 = read.csv("realDonaldTrump.csv", header=TRUE, sep=",")
# you could load them directly from those locations (assuming your computer can access the URLs), as a pre-caution,
#   the 2 datasets have been provided so you can load them from your working directory:
myTweets1 = read.csv("BarackObama.csv", header=TRUE, sep=",")
#add a new column (called "whichDataset") to each dataframe and assign the entire column a value
myTweets1$whichDataset = "Obama"
myTweets2$whichDataset = "Trump"
str(myTweets1)
summary(myTweets1)
# similar to how we used the "c" function, here we will use "rbind" to combine to dataframes
myTweets = rbind(myTweets1, myTweets2)
str(myTweets)
summary(myTweets)
# store the text column data in a variable - you will see how we use this shortly
myDataBeforeCleaning = myTweets$text
myDataBeforeCleaning
# store the text column data in a variable that we will use to score the sentiment of the data; this variable will
#   first pass through our generalCleaning function so that the state of data will be able to be compared to the
#   imported positive/negative lexicons:
myDataAfterCleaning = generalCleaning(myTweets$text)
myDataBeforeCleaning[42]
myDataAfterCleaning[42]
generalCleaning = function(dataToClean)
{
dataToReturn = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", dataToClean) # remove hyperlinks
dataToReturn = gsub("<.*?>", "", dataToReturn) # remove HTML
dataToReturn = gsub("[^[:graph:]]", " ", dataToReturn) # anything not alphabetic and punctuation (e.g. emojis, pictures, videos)
dataToReturn = gsub("[[:punct:]]", "", dataToReturn) # remove punctuation
dataToReturn = gsub("[[:cntrl:]]", " ", dataToReturn) # remove control characters
dataToReturn = gsub("\\d+", "", dataToReturn) # remove numbers
dataToReturn = gsub("[[:space:]]", " ", dataToReturn) #remove "white space" characters, noting " " vs ""
dataToReturn = gsub("“", "", dataToReturn) #remove specific characters
dataToReturn = gsub("‘", "", dataToReturn) #remove specific characters
dataToReturn = gsub("”", "", dataToReturn) #remove specific characters
dataToReturn = gsub("  ", " ", dataToReturn) #remove specific characters
dataToReturn = gsub("äóî", "", dataToReturn) #remove specific characters
dataToReturn = tolower(dataToReturn) # notice how we make it lowercase before returning the data
return(dataToReturn)
}
myDataBeforeCleaning = myTweets$text
myDataAfterCleaning = generalCleaning(myTweets$text)
myDataBeforeCleaning[42]
myDataAfterCleaning[42]
# pass the text from the tweets to the sentiment function for scoring
scores = calculateSentiment(myDataAfterCleaning, myDataBeforeCleaning, positiveWords, negativeWords, myTweets$whichDataset)
write.csv(scores, "sentiment.csv")
write.csv(scores, "sentiment.csv", row.names = FALSE)
buildTDM = function(inputData)
{
myCorpusData = VCorpus(VectorSource(inputData))
myCorpusData = tm_map(myCorpusData, removeWords, stopwords("english"))
myCorpusData = tm_map(myCorpusData, stemDocument)
return(TermDocumentMatrix(myCorpusData))
}
myTDMData = buildTDM(myDataAfterCleaning)
findFreqTerms(myTDMData, 150, Inf)
findFreqTerms(myTDMData, 300, Inf)
# notice the "äóî" - we could modify our generalCleaning function to targer and remove this data
findAssocs(myTDMData, term="american", corlimit=0.1)
# additionally, we can use our TDM to build a wordcloud for quick visualization:
freqOfTerms = sort(rowSums(as.matrix(myTDMData)), decreasing=TRUE)
# get a list of the words from the heading
namesFromData = names(freqOfTerms)
# show the importance of words based on frequency with a word cloud
wordcloud(namesFromData, freqOfTerms, min.freq = 150)
